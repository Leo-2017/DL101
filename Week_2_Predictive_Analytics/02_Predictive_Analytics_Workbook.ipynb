{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_Tabular_Workbook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unpackAI/DL101/blob/main/Week_2_Predictive_Analytics/02_Predictive_Analytics_Workbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j04QL-TPJy1"
      },
      "source": [
        "# ðŸ’»Week 2 Workbook of unpackAI \"DL101 Bootcamp\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0joGgV0POOt"
      },
      "source": [
        "## ðŸ“• Learning Objectives of the Week\n",
        "\n",
        "* Get hands-on experience on building tabular models "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx21fMyjgV1v"
      },
      "source": [
        "## unpackAI Assignment Section\n",
        "\n",
        " \n",
        "*   **Assignment 1**: Go through the multiple choice questions below and choose the correct answer. Discuss during the presentation session.\n",
        "*   **Assignment 2**: Build an entire classification model starting from defining your objective, gathering data to training your model and interpreting the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIYs4jA3yY7L"
      },
      "source": [
        "# Assignment 1: \n",
        "\n",
        "#### The questions here are designed to ensure that you can know the following\n",
        "\n",
        "*    Why a random forest is useful as a ML algorithm\n",
        "\n",
        "*    Understand the difference between a categorical and a continous variable\n",
        "\n",
        "*    Understand the reasons why features can or should be removed from a the model\n",
        "\n",
        "*    **Ensure that you understand the names of the different variables, functions, and classes in the code, so that you can edit and reuse them independently.**\n",
        "\n",
        "## The following questions are multiple choice to help guide undestanding of the content. However, there can be one or multiple answers to each of the questions.\n",
        "\n",
        "#### Please skim through the questions, and as you run through the notebooks fill in the answers as you find them.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdV2cZh0yY7N"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Section A**: Understanding the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bnPzzssyY7P"
      },
      "source": [
        "###1. What is a \"target variable? \n",
        "\n",
        "* A. It is similar to the y variable in an equation.\n",
        "* B. It informs the algorithm what to target during convolution data processing iterations.\n",
        "* C. It is the variable that our model seeks to predict.\n",
        "* D. It helps the algorithm decide how to split the data set\n",
        "\n",
        "\n",
        "\n",
        "### 2. Why do you need to split your dataset into training, validation and test sets?\n",
        "\n",
        "* A. The model can easily overfit on the training set, so validation and test are extra sets to avoid overfitting.\n",
        "* B. You train on the training set, you check the metrics on the validation set, you make the model predictions on the test set.\n",
        "* C. The validation set is used to check your assumptions before the model training, while the test set is used test the results of the model training.\n",
        "* D. The training set is required to train the baseline model, the validation set is used for model tuning, the test set is used to fine-tune the model until it achieves the state-of-the-art result.\n",
        "\n",
        "\n",
        "###3. After learning about AutoML, how would you approach machine learning tasks?\n",
        "\n",
        "* A. AutoML is inherently dumb thing, because you have to use the neural network algorithm in the most of the times as its very powerful in making predictions on sophisticated data.\n",
        "* B. AutoML allows you to explore how several algorithms might fit the data in order to determine the most suitable one.\n",
        "* C. AutoML is only for non-ML professionals because ML engineers know exactly what methods and techniques will work just by looking at the data.\n",
        "* D. AutoML is not easy at all as you still need to code AutoML steps, have strong mathematical and machine learning skills \n",
        "\n",
        "###4. Why would you cross validate the results of our training using many folds?\n",
        "\n",
        "* A.Splitting dataset to trainig/validation/test sets might not be enough as you might not have the same data distribution among all sets to confidentally determine ML model training results. \n",
        "* B.In case if you have a small dataset, you might have the validation set so small with only few examples of the particular class that will result in pure model evaluation\n",
        "* C. The similar model performance accross all folds might indicate that model will perform well on unseen data\n",
        "* D. Splitting the dataset randomly will not always result in randomness accross the trainining/validation/test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxwIc5jGyY7R"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "###**Section 2**: Understanding the difference between categorical and continous variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQOIpWGpyY7S"
      },
      "source": [
        "###5. Which of the following are properties of continuous variables?\n",
        "\n",
        "* A. They can be interpreted mathematically as data points along a line/plane\n",
        "* B. They are easy to identify whenever you see numbers in the dataset\n",
        "* C. They usually lie within a range and can be full integers (1,5,-7) or floating point decimals (88.01, 94.00) depending on how the data was collected\n",
        "* D. Their only purpose is to divide the dataset into easy to understand groups\n",
        "\n",
        "###6. Which of the following are properties of categorical variables?\n",
        "\n",
        "* A. They tell us information which doesn't have immediate mathematical meaning like colors, locations, or names.\n",
        "* B. They allow us to group the dataset into categories based on properties \n",
        "* C. They always appear as text in a dataset. \n",
        "* D. They exist along a quantifiable spectrum.\n",
        "\n",
        "###7. Which of the following should be treated as a continuous variable rather than a categorical variable?\n",
        "\n",
        "* A. E-commerce goods organized by their use/function on a website such as Amazon\n",
        "* B. Postal Codes and Phone Numbers from a customer database\n",
        "* C. Educational background (none, primary school, some secondary school, secondary school, some university, university, masters, doctorate, postdoc) \n",
        "* D. Randomly generated primary keys from a SQL database \n",
        "\n",
        "###8. Which of the following should be treated as a categorical variable rather than a continuous variable? \n",
        "\n",
        "* A. The standard deviation from the mean of a sample\n",
        "* B. The rolls of a 6 sided dice (1,2,3,4,5,6)\n",
        "* C. Temperature as reported by people (hot, cold, warm, freezing, cool)\n",
        "* D. The materials used in manufacturing a product (Plastic, Metal, Glass).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUbu_n7fiITe"
      },
      "source": [
        "## Assignment 2: Build an entire classification model starting from defining your objective, gathering data to training your model and interpreting the results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='red'>IMPORTANT!!! After you ran this codeðŸ‘‡, restart runtime to utilize a new library and re-run this cell to continue with the notebook  </font>"
      ],
      "metadata": {
        "id": "ssjDhJJ3g3h2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXn-UsEijVgI",
        "outputId": "204bf7bf-b3a9-47e6-d4e1-78aeebfb4231"
      },
      "source": [
        "import sys\n",
        "!pip install -Uqq pandas-profiling[notebook]\n",
        "!pip install -Uqq pycaret\n",
        "!jupyter nbextension enable --py widgetsnbextension\n",
        "!pip install markupsafe==2.0.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pandas-profiling 1.4.1 does not provide the extra 'notebook'\u001b[0m\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320 kB 4.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16.8 MB 39.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.8 MB 26.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 3.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0 MB 38.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 167 kB 56.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 262 kB 58.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88 kB 6.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 53.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118 kB 60.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 47.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690 kB 62.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102 kB 9.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.7 MB 40.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.9 MB 29.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636 kB 51.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 812 kB 47.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.9 MB 1.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 271 kB 18.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.8 MB 16.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62 kB 730 kB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79 kB 7.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 210 kB 57.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181 kB 57.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 146 kB 46.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54 kB 2.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63 kB 1.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78 kB 6.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 44.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 22.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 43.1 MB/s \n",
            "\u001b[?25h  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Enabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/root/.jupyter/nbconfig/notebook.json\n",
            "Collecting markupsafe==2.0.1\n",
            "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
            "Installing collected packages: markupsafe\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 2.1.1\n",
            "    Uninstalling MarkupSafe-2.1.1:\n",
            "      Successfully uninstalled MarkupSafe-2.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-profiling 3.2.0 requires markupsafe~=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed markupsafe-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dSDbNqWzpjg",
        "outputId": "242ea567-3bff-4e51-e5f2-971d723a60d4"
      },
      "source": [
        "# Standard Library Imports\n",
        "from pathlib import Path\n",
        "\n",
        "# Installed packages\n",
        "import pandas as pd\n",
        "from ipywidgets import widgets\n",
        "\n",
        "# Our package\n",
        "from pandas_profiling import ProfileReport\n",
        "from pandas_profiling.utils.cache import cache_file\n",
        "\n",
        "#Enable pycaret on Colab\n",
        "from pycaret.utils import enable_colab\n",
        "enable_colab()\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab mode enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMjo3Bh63l-a"
      },
      "source": [
        "#### 0. Define a machine learning task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-prUftUtT_u"
      },
      "source": [
        "Try to look first at the coursebook's example in order to articulare your machine learning task, what's observed and what answer you want the model to predict. \n",
        "\n",
        "**Machine Learning Task**: \n",
        "\n",
        "**Target**: \n",
        "\n",
        "**Samples**: .\n",
        "\n",
        "**Features**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OFx2kTakt2E"
      },
      "source": [
        "#### 1. Choose your dataset\n",
        "\n",
        "We highly recommend you to search for your own Tabular Data dataset. For that you can easily go on [Kaggle](https://www.kaggle.com/) and search for your own tabular classification dataset.\n",
        "\n",
        "Examples could be \n",
        "\n",
        "1. [Online shopper purchasing intention:](https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset) Will the shopper purchase or not purchase a product?\n",
        "1. [Mushroom Classification](https://www.kaggle.com/uciml/mushroom-classification): Edible or poisonous mushroom?\n",
        "1. [Others...](https://www.kaggle.com/datasets?search=classification&datasetsOnly=true)\n",
        "\n",
        "In order to  utilise the dataset, you will have to download it, and upload it into the Colab Notebook using the files feature on the left. See the image below for the step-by-step process.\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/rmo90567wwjohxp/Upload.png?dl=1\" width=\"900\"/>\n",
        "\n",
        "As a data requirement, make sure that you are working with a tabular data classification task. The data format has to be `csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkUg9HF3B-Vi"
      },
      "source": [
        "#### 2. Prepare the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdhVpxhgsrQ"
      },
      "source": [
        "You can build DataFrame directly by reading csv file from download link we provided in the Assignment description.\n",
        "\n",
        "Tips: use `pd.read_csv(\"your path goes here\")`\n",
        "\n",
        "> Make sure to not forget the \"\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih8V-H1buLP0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5sG0hdDhYXH"
      },
      "source": [
        "#### 3. Explore your dataset using Pandas Profiling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEYwJ4XfBXC8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fgUqsZwkz-L"
      },
      "source": [
        "Split the dataset to training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QpS48Jgk5DN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1-rEuOWr2QP"
      },
      "source": [
        "#### 4. Automated Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoRU_z5qLcxL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}