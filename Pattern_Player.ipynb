{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pattern Player.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leo-2017/DL101/blob/main/Pattern_Player.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j04QL-TPJy1"
      },
      "source": [
        "# Pattern Player üéπ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction**"
      ],
      "metadata": {
        "id": "mHvkbz5e85md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What business problem are you solving?\n",
        "Too much stress and bullshit people have to experience from work and lockdown situation. When they‚Äôre are home, they can us their own designed wallpaper in different rooms to reduce stress and make them feel calm when they get home from work; \n",
        "Yet hiring a designer is too expensive; \n",
        "\n",
        "A program for non-coders to get started with machine learning development. The course contains flexible and powerful ML pipelines for computer vision, tabular, recommender systems and NLP problems"
      ],
      "metadata": {
        "id": "lb8S7bqY3na3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. How can AI/ML technology help to solve that?\n",
        "A trained language model can \n",
        "text generator <br> \n",
        "Text Generation in Natural Language Processing\n",
        "<br>\n",
        "self-supervised learning\n",
        "<br>\n",
        "A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence."
      ],
      "metadata": {
        "id": "vF6Jt_Re4Vzj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0joGgV0POOt"
      },
      "source": [
        "## 3. What is your AI project's objective?\n",
        "\n",
        "\n",
        "The objective A language model can predict the probability of the next word in the sequence, based on the words already observed in the sequence.\n",
        "\n",
        "The objective is to build a language model as a pattern design generator that helps user to figure out what kind of wallpaper they would like to have.\n",
        "\n",
        "\n",
        "Our Solution is a Language Model that uses the GPT-2 model (pretrained on a dataset of 8 million web pages) as a foundation, and our dataset above to fine-tune the model and make it more applicable for our use case."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Who are the users of your AI application and how will they interact with it? "
      ],
      "metadata": {
        "id": "K1swq9pu4gPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Benchmark Study**"
      ],
      "metadata": {
        "id": "0oEKZdvo90SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar existing AI solutions\n",
        "\n",
        "Use Case 1 <br>\n",
        "**Delle-mini** <br> \n",
        "https://huggingface.co/spaces/dalle-mini/dalle-mini <br>\n",
        "https://www.craiyon.com/ <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-tCbRD5-V9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Data**"
      ],
      "metadata": {
        "id": "BSuva5jzIbIL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJvIFM7zjGnj"
      },
      "source": [
        "## 1. Libraries & Dependencies needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azcl-ikfOQJR",
        "outputId": "6eca831b-fb20-4e50-c537-c4ed8c4c31d6"
      },
      "source": [
        "!pip install -Uqq unpackai\n",
        "# !pip install -q git+https://github.com/unpackai/unpackai\n",
        "!pip install -Uqq transformers==4.10.2\n",
        "!pip install -q datasets transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73 kB 2.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42 kB 1.3 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 976 kB 58.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.8 MB 27.3 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 56.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.3 MB 52.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101 kB 13.0 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 880 kB 65.5 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 362 kB 33.2 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140 kB 72.2 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 60.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212 kB 61.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 127 kB 60.4 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144 kB 75.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94 kB 3.2 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271 kB 73.4 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 41.3 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynOZVJ0HfD2Q"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    pipeline,\n",
        "    set_seed,\n",
        "    Trainer,\n",
        "    TextDataset,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "from unpackai.nlp import Textual, InterpEmbeddingsTokenizer\n",
        "from ipywidgets import interact\n",
        "import logging\n",
        "from fastai import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUbu_n7fiITe"
      },
      "source": [
        "## 2: Build an Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1PTaXRGQKxS"
      },
      "source": [
        "### Step One - Define a ML problem and propose a solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDEouac7xN1v"
      },
      "source": [
        "#### 1. Define the objective\n",
        "**Your objective:** Build a Language Model than can create the text of a fiction novel.\n",
        "#### 2. Describe your dataset\n",
        "**Your dataset:** The Return of Sherlock Holmes by Arthur Conan Doyle. 107,148 words. Gutenberg txt file.\n",
        "#### 3. Describe your model\n",
        "**Your model:** Transformer Model to generate text it was pretrained on, based on an initial set of words that I provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5QZmieCYbZ2"
      },
      "source": [
        "### Step Two - Collect and construct your dataset\n",
        "\n",
        "In order to collect and design your own dataset we provide you with the `Textual` scraping tool below. For that you have two options!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Uma7pnjrm5"
      },
      "source": [
        "**Option 1: Collect text from an URL.**\n",
        "\n",
        "Provide an URL to the textual tool and all the text will be collected. We recommend [Gutenberg Project](https://gutenberg.org/) as a source which is a collection of over 60,000 free eBooks. Simply click on a book that you like and the corresponding txt file (see image below).\n",
        "\n",
        "<img src=\"https://www.dropbox.com/s/p5tyklp58d3yosa/Screen%20Shot%202021-09-13%20at%2017.49.52.png?dl=1\" alt=\"wordembeddings\" width=\"500\"/>\n",
        "\n",
        "However, feel free to experiment with other datasets from other URL sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCWwVhIAjjBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a3c298-4e27-4dec-a670-a6f12aa927ce"
      },
      "source": [
        "textual = Textual.from_url(\"https://gutenberg.org/cache/epub/36250/pg36250.txt\")\n",
        "textual"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text (393777 chars), textual(),\n",
              "    train_path, val_path = textual.create_train_val()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textual = Textual.from_url(\"https://gutenberg.org/cache/epub/39749/pg39749.txt\")\n",
        "textual"
      ],
      "metadata": {
        "id": "kCmdqDOKT7cs",
        "outputId": "67202664-8e03-425b-baf5-4fc23257e129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text (414489 chars), textual(),\n",
              "    train_path, val_path = textual.create_train_val()"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textual = Textual.from_url(\"https://gutenberg.org/cache/epub/17531/pg17531.txt\")\n",
        "textual"
      ],
      "metadata": {
        "id": "DVd2OsqmUng9",
        "outputId": "41f163a8-ab00-4fac-c698-07fa80e1d3b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text (80332 chars), textual(),\n",
              "    train_path, val_path = textual.create_train_val()"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textual = Textual.from_url(\"https://gutenberg.org/files/60034/60034-0.txt\")\n",
        "textual"
      ],
      "metadata": {
        "id": "XmRXdGqSU80z",
        "outputId": "611c804a-78f3-475e-f91e-b9a28c078131",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text (234063 chars), textual(),\n",
              "    train_path, val_path = textual.create_train_val()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1iDa9dplr2h"
      },
      "source": [
        "**Option 2: Collect text from path.**\n",
        "\n",
        "In case you have a downloaded txt file, feel free to upload it to the Google Drive and enter the path in the bracket below. Make sure that the file format is txt (a type of file just like ppt or csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TzPtIt0jpZc",
        "outputId": "538d9da1-2ba3-44e7-9133-ac8be90b6b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "source": [
        "textual = Textual.from_path(\"./the_txt_file_you_uploaded.txt\")\n",
        "textual"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div>\n",
              "    <html>\n",
              "    <head>\n",
              "        <meta name=\"viewport\"\n",
              "        content=\"width=device-width, initial-scale=1\">\n",
              "    </head>\n",
              "    <body>\n",
              "        <a download=\"npakai_FileExistsError_0713_154259.html\" href=\"data:text/html;base64,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\" >\n",
              "    <button class=\"p-Widget jupyter-widgets jupyter-button widget-button mod-success\">\n",
              "        ü¶ã Download Report</button>\n",
              "    </a>\n",
              "    </body>\n",
              "</html> <span>Download error report and consult a mentor/TA</span>\n",
              "</div>\n",
              "<h2>üêû [FileExistsError] Happened, Don't Panic</h2>\n",
              "\n",
              "<h3>ü§î Default message:</h3>\n",
              "<hr>\n",
              "<quote>\n",
              "    Cannot find the_txt_file_you_uploaded.txt\n",
              "</quote>\n",
              "<hr>\n",
              "<h3>Traceback Info</h3>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[0;32m\"<ipython-input-5-7af509265d33>\"\u001b[0m, line \u001b[0;32m1\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n",
            "    textual = Textual.from_path(\"./the_txt_file_you_uploaded.txt\")\n",
            "\u001b[1;36m  File \u001b[1;32m\"/usr/local/lib/python3.7/dist-packages/unpackai/nlp/data.py\"\u001b[1;36m, line \u001b[1;32m59\u001b[1;36m, in \u001b[1;35mfrom_path\u001b[1;36m\u001b[0m\n",
            "\u001b[1;33m    raise FileExistsError(f\"Cannot find {path}\")\u001b[0m\n",
            "\u001b[1;31mFileExistsError\u001b[0m\u001b[1;31m:\u001b[0m Cannot find the_txt_file_you_uploaded.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYOIkqBuZAm5"
      },
      "source": [
        "### Step Three - Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z3TG7_8mZJU"
      },
      "source": [
        "We skip the Data Transformation as the Tokenization and Numericalisation happens during the Model Training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnzDFAZZZXCf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667421d8-9794-488b-c72d-0099c9e88015"
      },
      "source": [
        "pretrained_model = pipeline(\"text-generation\", model='gpt2')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXFzPxONZGu1"
      },
      "source": [
        "### Step Four - Interpret the model and generate text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDmQ23JmZWrn",
        "outputId": "0e066518-5f56-43cc-a92e-0c310bfbec34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "arguments = TrainingArguments(\n",
        "    output_dir=\"./write_style\",\n",
        "    overwrite_output_dir=True,  \n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=24,\n",
        "    per_device_eval_batch_size=64,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = textual.get_hf_trainer(\n",
        "    model=pretrained_model.model,\n",
        "    tokenizer = pretrained_model.tokenizer,\n",
        "    arguments = arguments\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwUUSAVq1k52",
        "outputId": "5affb10a-6809-4eb5-f096-e72ebf5fa3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Loading features from cached file ./cached_lm_GPT2TokenizerFast_128_train_text.txt [took 0.004 s]\n",
            "Loading features from cached file ./cached_lm_GPT2TokenizerFast_128_val_text.txt [took 0.001 s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "jQYmFADe12YS",
        "outputId": "4e57699f-c435-49a3-8289-7024bdb197f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 573\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 24\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 72\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='72' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72/72 00:54, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=72, training_loss=3.598634507921007, metrics={'train_runtime': 54.9901, 'train_samples_per_second': 31.26, 'train_steps_per_second': 1.309, 'total_flos': 112290250752000.0, 'train_loss': 3.598634507921007, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnCZG4xy2whe",
        "outputId": "66676245-45d0-41bb-faba-945bbd0ad884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to ./write_style\n",
            "Configuration saved in ./write_style/config.json\n",
            "Model weights saved in ./write_style/pytorch_model.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_writer = pipeline('text-generation',\n",
        "                        model='./write_style',\n",
        "                        tokenizer=pretrained_model.tokenizer\n",
        "                        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahuEatAN22ET",
        "outputId": "01a87ceb-5736-49e0-ed1f-a37479e7f7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file ./write_style/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"do_sample\": true,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"max_length\": 50,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading configuration file ./write_style/config.json\n",
            "Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"do_sample\": true,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"max_length\": 50,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "loading weights file ./write_style/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./write_style.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_writer(\"How to decorate my wall with botanical ornaments?\",\n",
        "            max_length=200,\n",
        "            num_return_sequences=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcuHOsd23TO3",
        "outputId": "f7b77798-a132-4713-c94f-fb82999ed8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'How to decorate my wall with botanical ornaments? I take great pains to make it well-stocked with all kinds of furniture suitable for use. Many artists who have succeeded themselves in doing an artistic thing have brought home so many important objects that the beauty of the designs can hardly be denied--or can they be claimed to have done without any artistic intention at all? We can, of course, hardly understand the importance of these things if we are not interested in them. How much more important is they than that of furniture, with whom we must rely all our leisure and energy, when workmanship is as much necessary and indispensable to any artistic work as it was even sixteenth-century furniture!  Thus long as these people still do not learn enough to distinguish the art from the skill of carving, and to appreciate the artistic character of the parts they are attempting to imitate, they will soon find themselves unable to appreciate the qualities of the finished work, and when this task is over'}]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_writer(\"How to decorate my bedroom wall with blossom pattern ornaments\",\n",
        "            max_length=200,\n",
        "            num_return_sequences=3)"
      ],
      "metadata": {
        "id": "RFvXSwU7WWu-",
        "outputId": "7ff83a71-2215-4205-bb58-bbfc0d6c0e3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'How to decorate my bedroom wall with blossom pattern ornaments, and how will its ornament be affected by the style of the designer?     With the help of Dr. Kekri, I set out, on the 18th of May, from Strasbourg.      This period is especially important since the English Renaissance is the last in which the French and Italian churches are entirely devoted to the study of painting.   Our little studio in the neighbourhood of Strasbourg was the object of the study of two Italian artistes: Lorenzo Maggio, and Maria Corrado, and the works of her sister-in-law Francesco Aarion da Figaro-Coppini, both in fine details and in a rich, Gothic pattern, were also displayed, in the chapel of the Christian Church.     Now the Gothic work of Giuseppe Veneto has been adapted, and the Gothic art of Milan and Paris has been rendered'},\n",
              " {'generated_text': 'How to decorate my bedroom wall with blossom pattern ornaments, as my friend J. H. Johnson writes, is an essential craft for this country, and of which it is an admirable object.   The most obvious and practical method for making lace, which looks as though it were applied to cottonwood or stonewall, is to turn it upside down or up.  This means using a circular point on the bottom of a fluted or silk pillow, and placing that point at the top of the fluted line, and so on until the fabric appears at or above the point where the stitches shall meet, and at the bottom of the line, using the same thread over and over again, cutting it in three or four steps, and then twisting down again a bit, putting the ends of the threads with their ends back in this way, until the end of the lace is almost round round, and when it is finished they are all in place. If you are going'},\n",
              " {'generated_text': 'How to decorate my bedroom wall with blossom pattern ornaments? I can take quite a bit of imagination out of my design. In general what makes decoration worth doing is the opportunity to give it character, not the actual composition of it. I\\'m not a big fan of putting together a little decorative book-helve, like a book-case or a bookcase that might give way to a book-case, which is, in fact, too small for myself; yet, as I said, I feel fairly certain that it is more suitable to be done in the space or in the way as a book. \"I sometimes think the ornament of the furniture that we live in is as much in danger from the danger of not making the best use of the space I think we have in it, but the danger of leaving us the space that is necessary for our rooms: and at this moment I feel quite sure that the best way might be to set together this space with a general'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "style_writer(\"How to decorate my bedroom wall with cyclamen persicum pattern?\",\n",
        "            max_length=200,\n",
        "            num_return_sequences=1)"
      ],
      "metadata": {
        "id": "5HoS8FVUX0qf",
        "outputId": "8bbe36e1-d86e-4544-b060-4b7a1d64f69c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'How to decorate my bedroom wall with cyclamen persicum pattern?   I will show you how to do it, and I\\'ll give you a little explanation, for myself, as well as a picture which seems almost necessary to illustrate the task. I have three examples--one of these a small and another very large--and I am the first to see that it will be in style as well as useful, which the first of the three is quite clear in its simplicity. For the first-mentioned, as has been mentioned, it is a kind of lace, as all lace has its characteristic colours of pink, a crimsonish hue, or red, the colour of which is the first thing you see--this is the colour most suited to lace-work, of course, by itself; but then, of course, what to call it is an artificial or \"strict\" lace, being a material of very thin and unkempt threads, like that of some furniture.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}